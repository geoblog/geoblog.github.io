<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[台风“玛娃”生成，3日将登陆广东东部沿海]]></title>
    <url>%2F2017%2F08%2F31%2Fgeo-knowledge%2F</url>
    <content type="text"><![CDATA[来源： 广东省气象公共服务中心 一、台风动态9月1日02时,原位于南海东北部海面的热带低压加强为今年第16号台风“玛娃”（热带风暴级，英文名：MAWAR，名字来源于马来西亚，意为玫瑰花），08时其中心位于汕尾市东南方向约460公里的南海东北部海面上，也就是北纬19.6度、东经118.5度，中心附近最大风力8级（18米/秒，相当于65公里/小时），中心最低气压998百帕。 预计，“玛娃”将以10公里左右的时速向偏西北方向移动，强度加强，并趋向广东东部海面，可能于3日早晨到中午以台风级（12级～13级）或强热带风暴级（11级）在深圳到汕头沿海地区登陆。台风“玛娃”生成，3日将登陆广东东部沿海根据《广东省气象灾害应急预案》，广东省重大气象灾害应急办公室已于9月1日7时启动气象灾害（台风）Ⅳ级应急响应。 二、天气预报预计，9月2日夜间-4日广东将有一次明显的风雨过程。具体预报如下：9月1日-2日白天，粤北和珠江三角洲市县多云为主，有（雷）阵雨，部分市县伴有8级或以上雷雨大风；其余市县多云间晴有分散（雷）阵雨。粤北、粤东和珠江三角洲市县有35℃的高温。9月2日夜间-4日，粤东市县、梅州、河源和韶关市有暴雨到大暴雨，局部特大暴雨；珠江三角洲东部市县有暴雨；其余市县有大雨局部暴雨。另外，9月1日-3日，南海北部海面风力逐渐加大到10-12级、阵风13级；9月2日-3日，广东中东部海面和沿海市县风力将逐渐加大到10级～12级、阵风13级～14级。 三、关注和建议“玛娃”移速较慢、近海加强，又恰逢在周末登陆广东，需迅速做好防御： 南海北部和广东沿海海面风力将逐渐加大，海上过往船只和作业人员要及时回港避风；海岛旅游和水上项目需注意安全，适时关闭。 沿海市县需做好建筑工棚、人工构筑物、户外广告牌、道路绿化树木等的防风加固工作。 需防御台风带来的局地强降水引发的城乡积涝、山洪、泥石流、山体滑坡等灾害。 高温背景下广东易发局地强雷雨，需注意防御局地雷击和短时雷雨大风等导致的灾害。南海台风强度和路径多变，请密切关注有关“玛娃”的最新监测和预报。 天气多变，关注“广东天气”官方微博，或“广东天气”微信中“缤纷微天气”栏目，可随时查看最新实况、预报预警信息。 转载请注明出处。广东省气象公共服务中心]]></content>
      <tags>
        <tag>天气</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA['重装系统一定要注意保存数据']]></title>
    <url>%2F2017%2F07%2F25%2F2017-07-25%2F</url>
    <content type="text"><![CDATA[昨天重装了系统，有较多数据丢失了，可惜。很多软件也得重装……]]></content>
  </entry>
  <entry>
    <title><![CDATA[地理百科知识竞赛（一）]]></title>
    <url>%2F2017%2F07%2F12%2F20170712%2F</url>
    <content type="text"><![CDATA[地理百科知识竞赛参考资料，大家感兴趣的可以看一看 出自《孟子》的“生于忧患”，下一句是？ 死于安乐 世界经济论坛“达沃斯论坛”位于欧洲哪个国家？ 瑞士 女扮男装替父从军的花木兰家乡在我国哪个省？ 河南 国际正式足球比赛中，上下半场各位多少时间？ 45分钟 金庸小说《鹿鼎记》里面的主角韦小宝的妈妈叫什么？ 韦春花 在太阳系八大行星中，那一颗行星离地球最近？ 金星 歌曲《你是我心内的一首歌》是王力宏和SHE中哪位成员合唱的？ Selina/任家萱 隋朝的隋字由多少笔画组成？ 十一画 杜甫《春夜喜雨》中“随风潜入夜”的下一句？ 润物细无声 “西气东输”指的是把那个盆地的天然气输给长江三角洲地区？ 塔里木盆地 “当初是你要分开，分开就分开”这是哪首流行歌曲的歌词？爱情买卖 俗话说“人要脸”的下一句? 树要皮 常用“精神胜利法”来自我安慰的是鲁迅笔下的那个人物？ 阿Q 刘玉玺的诗歌《竹枝词》中“东边日出西边雨”的下一句？ 道是无情却有情 演唱的电影《画皮》主题曲《画心》的内地女歌星叫什么？ 张靓颖 “树上的鸟儿成双对，绿水青山的笑颜”是出自那部经典黄梅戏选段？ 天仙配（七仙女下凡） 平面任意多边形外角和是多少度？ 360度 游泳池的水中，因为加入了硫酸铜，池中呈现的是什么颜色？ 蓝色 新加坡的国徽上是一只狮子和一只什么动物？ 老虎 大气中的那种气体可以大量吸收紫外线？ 臭氧 中国第一个奥运会冠军是谁？许海峰 负荆请罪讲的是谁和丞相蔺相如的故事？廉颇 请问生物进化论的创始人是谁？ 达尔文 万有引力定律是谁发现的？ 牛顿 请问谁发明了安全炸药？ 诺贝尔 亚马逊河在哪里？ （巴西） 世界哪个国家的斗牛活动首屈一指 ? ( 西班牙) 人在死海里会浮还是沉？ 会浮在水面上 成语“冰冻三尺”的下句是？ 非一日之寒 二战过后，审判法西斯的德国法庭在哪一个城市？纽伦堡 电视剧《甄嬛传》的“甄嬛”的扮演着是哪位女明星？ （孙俪） 作家鲁迅的原名叫什么？ （周树人） “三过家门而不人”是哪一历史人物的故事？ 禹 我国第一大岛是: 台湾岛 于 2012 年提出破产保护的原世界最大的照相机，胶卷生产供应商是哪家？柯达 古人强调尊师重道，常说的“一日为师”的下一句是什么？ 终生为父 在我国可兑换的国际通用外币中，最值钱的是哪个币种？英镑 收藏有油画《蒙娜丽莎》等珍贵作品的法国博物馆是哪一座？卢浮宫 解放战争时期著名的“三大战役”中第一个打响的是哪个战役？辽沈战役 国际上用“K”表示含金量，18k 金表示含金量百分比为百分之多少？75% 年作为央视春节联欢晚会结尾曲的歌曲叫什么名字？难忘今宵 历史上“焚书坑儒”的只中国的那一位皇帝？秦始皇 中国古代名医华佗为谁所杀? 曹操 爱国华侨陈嘉庚出资兴建的大学是哪一所?—厦门大学 著名风景区九寨沟位于我国哪个省? ( 四川) 奢侈品牌 LV 路易威登是哪个国家的品牌？法国 被称为捕虫神刀手的是什么动物？ 螳螂 中国的“导弹之父”是谁？ 钱学森 为什么先看见闪电后听到雷声？ （光波在空气中的传播速度比声速快） 奥运会上首次只有女性可以参加的比赛项目，是花样游泳和什么项目？艺术体操 成语“人无千日好”的下一句？ 花无百日红 传统的“金砖四国”指的是俄罗斯、中国、印度和哪个国家？ 巴西 占据我国陆地总面积四分之一的高原是那个？ 青藏高原 nba篮球比赛中，正常那个情况下，全场一共分为几节进行？ 4节 在电视剧《笑傲江湖》中，日月神教的圣姑，也是任我行的女儿是谁？ 任盈盈 在太阳系八大行星中，质量和体积最小的是那一颗？ 水星 “在千山万水人海相遇，原来你也在这里”是哪位女歌手歌曲中的歌曲？ 刘若英 翻滚的翻字由多少笔画组成？ 十八画 在第六次人口普查公布数据中，我国人口最多的省是哪个省？ 广东省 “南水北调”中的东线工程是利用哪个省的江水进行北调？ 江苏]]></content>
      <tags>
        <tag>地理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[中国高铁“四纵四横”到“八纵八横”]]></title>
    <url>%2F2017%2F07%2F10%2F20170710train%2F</url>
    <content type="text"><![CDATA[视频学习刚才QQ推送，介绍了中国铁路的进步，大家可以学习一下哦。iframe标签用不了，可能是对HTTPS的支持不够。 另，在阿里云注册了域名geostar.cc，把域名解析到github.io即可实现自定义域名。]]></content>
      <tags>
        <tag>地理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[20170710]]></title>
    <url>%2F2017%2F07%2F10%2F2017semi%2F</url>
    <content type="text"><![CDATA[订个目标吧好久没写BLOG了，都快忘记了怎么用HEXO了。今天整理了一下思路，给下半年做个计划。MARKDOWN也快忘记怎么用了。。。还好有MarkdownPad… 把BLOG写起来，记录一些知识碎片。 添加一些地理知识类的分享。 LIFE IS SHORT ,YOU NEED PYTHON.]]></content>
  </entry>
  <entry>
    <title><![CDATA[2017-04-15tensorflow]]></title>
    <url>%2F2017%2F04%2F15%2F2017-04-15tensorflow%2F</url>
    <content type="text"><![CDATA[准备学习GPU版TENSORFLOW及TENSORLAYER的神经网络技术。计划安装linux、WIN10双系统，开源就是力量哈哈。pycharm及anaconda是必备选项。做业务还是用WIN系统比较方便，LINUX做开发用吧。编辑PyCharm安装目录下PyCharm 4.5.3\bin下的pycharm.exe.vmoptions 修改CACHE SIZE。 F——testfrom sklearn.feature_selection import f_regressionreturn f score,p pearonrfrom scipy.stats import pearsonrreturn r,p]]></content>
  </entry>
  <entry>
    <title><![CDATA[2017-04-05ML_blending]]></title>
    <url>%2F2017%2F04%2F05%2F2017-04-05ML-blending%2F</url>
    <content type="text"><![CDATA[机器学习算法比较多，在同一个问题上，不同算法的性能有差异，Kaggle上的高手们推荐采用ensemble方法提高模型效果，试着看了一些他们推荐的方法，采用blending方法，确实效果有所提高。An ensemble method combines the predictions of differentalgorithms (the ensemble) to obtain a final predictionEach single algorithm models the data ina different way. Therefore a suitable combination of thesepredictions can significantly improve the prediction performance.Jahrer M, Töscher A, Legenstein R. Combining predictions for accurate recommender systems[C]// ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2010:693-702.]]></content>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2017-03-21kaggle]]></title>
    <url>%2F2017%2F03%2F21%2F2017-03-21kaggle%2F</url>
    <content type="text"><![CDATA[用notebook在线编码还是比较方便的。用各种回归方法进行测试拟合，训练集的性能还可以，但是用样本外的数据测试时R2及MSE\MAE所表现的性能较差。用365天、180天、90天数据进行训练，发现样本越多，测试集上的效果却越差。尝试feature engineering尝试stacking]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2017-03-13notebook]]></title>
    <url>%2F2017%2F03%2F13%2F2017-03-13notebook%2F</url>
    <content type="text"><![CDATA[今天想一想，如果把通过一个月数据训练出来的模型去检验另一个月的数据，效果会怎么样呢？r_squre of etr for Feb. is 0.389003895043r_squre of rfr is 0.300086648874r_squre of gbr is 0.336271301841r_squre of abr is -0.00792153810961r_squre of dis_knr is 0.026832730496r_squre of rbf_svr is 0.439851387112rbf_svr的效果最好如果把这几个r2_score在0.8以上的模型的数据集合起来做一个平均，会有所提高吗？ print(‘rsqure of avg is ‘,r2_score(y2,y_avr)) print(‘mean_squarederror of avg is ‘,mean_squared_error(ss_y.inverse_transform(y2),ss_y.inverse_transform(y_avr))) 使用etr,rfr,grb,xgb,svr rbf,svr linear,用PIPELINE简化代码，把多个模型的结果组合起来。在线流程图https://www.processon.com/diagramsplam 0.006768u10m 0.008958o3o3_graces 0.009357uwnd1000 0.009961coco_graces 0.010268vwnd700 0.012936no2c_graces 0.013694vwnd850 0.014014uwnd850 0.014316rh2m 0.014526uwnd925 0.015299mslp 0.016338shum500 0.016538hght1000 0.017211so2c_graces 0.017572v10m 0.017793hght925 0.018725uwnd700 0.018736shum700 0.019065shum1000 0.019533vwnd1000 0.020989t2mm 0.021139uwnd500 0.022543vwnd925 0.022681vwnd500 0.022957hght850 0.023828hght500 0.026042temp700 0.026952temp500 0.029715pm10_graces 0.031843temp850 0.034730temp1000 0.036947pm25_graces 0.048796cpre 0.049767hght700 0.052401temp925 0.064667shum850 0.075557shum925 0.096839]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2017-03-10kaggle]]></title>
    <url>%2F2017%2F03%2F10%2F2017-03-10-kaggle%2F</url>
    <content type="text"><![CDATA[今天用notebook matplotlib和seaborn对一组示例数据进行可视化分析，发现有必要对各目标变量进行LOG处理，处理前多为非正态分布，处理后可能会对预测能力有所提高。对各因素进行相关分析后发现有许多自相关的可以去掉其中一个。 &gt;&gt;&gt; np.log([1, np.e, np.e**2, 0]) array([ 0., 1., 2., -Inf]) 对训练数据进行处理，对目标数据可以不用，因为预测时不可能对目标数据进行处理。把CPRE换成[0，1]的dummy数据。 把两个LIST组合起来，然后排序，其中一个LIST是数值。用ZIP组合，再转成DICT，再用PD的series排序 f_importance=pd.Series(dict(list(zip(column,etr.feature_importances_,)))).sort_values() 当训练数据为10天以内里线性方法的R_squre为0.16，当数据增加到30天时，R_squre为0.56，训练数据的增加提高了预测的R_squre。]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2017-03-08 kaggle]]></title>
    <url>%2F2017%2F03%2F08%2F2017-03-08kaggle%2F</url>
    <content type="text"><![CDATA[模型评分函数sklearn.metrics.r2_score(y_true, y_pred, sample_weight=None, multioutput=None) R^2 (coefficient of determination) regression score function. Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a R^2 score of 0.0. Notes This is not a symmetric function.Unlike most other scores, R^2 score may be negative (it need not actually be the square of a quantity R). 用了LOG函数处理SO2后，分布变得NORM]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kaggle数据清洗]]></title>
    <url>%2F2017%2F03%2F07%2Fkaggle%2F</url>
    <content type="text"><![CDATA[学习机器学习回归之数据清洗。把非正态分布的数据变为正态分布可用LOG函数。把零与非零数据区分开来可新建一个二值变量[0，1]，选择非零的部分与预测目标进行相关分析，最后处理为dummy variables #convert categorical variable into dummy df_train = pd.get_dummies(df_train) ReferencesHair et al., 2013, Multivariate Data Analysis, 7th Edition (https://www.amazon.com/Multivariate-Data-Analysis-Joseph-Hair/dp/0138132631)数据处理：特征筛选，把因子中两两相关的只取其一，MISS DATA处理（删除或填补），LOG正态化，分类数据dummy处理,PCA降维(np.cumsum(pca.explained_varianceratio)=1时的主成分数就是目标参数) pca = PCA(whiten=True) pca.fit(data) variance = pd.DataFrame(pca.explained_variance_ratio_) np.cumsum(pca.explained_variance_ratio_) 选择多种算法，取效果最好的那一种。再GRIDSERCHCV寻找最佳参数。线性回归，SVR，集成方法，以KAGGLE社区上代码为例https://www.kaggle.com/miguelangelnieto/house-prices-advanced-regression-techniques/pca-and-regression/notebook def lets_try(train,labels): results={} def test_model(clf): cv = KFold(n_splits=5,shuffle=True,random_state=45) r2 = make_scorer(r2_score) r2_val_score = cross_val_score(clf, train, labels, cv=cv,scoring=r2) scores=[r2_val_score.mean()] return scores clf = linear_model.LinearRegression() results[&quot;Linear&quot;]=test_model(clf) clf = linear_model.Ridge() results[&quot;Ridge&quot;]=test_model(clf) clf = linear_model.BayesianRidge() results[&quot;Bayesian Ridge&quot;]=test_model(clf) clf = linear_model.HuberRegressor() results[&quot;Hubber&quot;]=test_model(clf) clf = linear_model.Lasso(alpha=1e-4) results[&quot;Lasso&quot;]=test_model(clf) clf = BaggingRegressor() results[&quot;Bagging&quot;]=test_model(clf) clf = RandomForestRegressor() results[&quot;RandomForest&quot;]=test_model(clf) clf = AdaBoostRegressor() results[&quot;AdaBoost&quot;]=test_model(clf) clf = svm.SVR() results[&quot;SVM RBF&quot;]=test_model(clf) clf = svm.SVR(kernel=&quot;linear&quot;) results[&quot;SVM Linear&quot;]=test_model(clf) results = pd.DataFrame.from_dict(results,orient=&apos;index&apos;) results.columns=[&quot;R Square Score&quot;] results=results.sort(columns=[&quot;R Square Score&quot;],ascending=False) results.plot(kind=&quot;bar&quot;,title=&quot;Model Scores&quot;) axes = plt.gca() axes.set_ylim([0.5,1]) return results sklearn.linear_model.HuberRegressor Fit Ridge and HuberRegressor on a dataset with outliers.The example shows that the predictions in ridge are strongly influenced by the outliers present in the dataset. The Huber regressor is less influenced by the outliers since the model uses the linear loss for these. As the parameter epsilon is increased for the Huber regressor, the decision function approaches that of the ridge. https://www.kaggle.com/jimthompson/house-prices-advanced-regression-techniques/ensemble-model-stacked-model-example/notebook To fit within the constraints of Kaggle’s Kernel offering, a simplified structure for the stacked model was used in this report. The specific simplications are Limit Level 0 to three modelsLImit Level 1 to one modelImprovements in stacked model performance can be accomplished by Adding models to Level 0 and Level 1 using different algorithmsTuning model Hyper-parametersAdding feature sets by feature engineeringAdding levels in the model structureFor additional information on model stacking see these references: MLWave: Kaggle Ensembling GuideKaggle Forum Posting: StackingWinning Data Science Competitions: Jeong-Yoon Lee This talk is about 90 minutes long. The sections relevant to model stacking are discussed in these segments (h:mm:ss to h:mm:ss): 1:05:25 to 1:12:15 and 1:21:30 to 1:27:00. 现有数据应该分为训练集和测试集，训练集拟合前应该打乱顺序时免受到时间序列的影响。交叉验证选择最优参数，然后在测试集上评分。然后 选择最高分的模式作为最终模式 或者把多个高分模式的结果进行平均，得到一个平均数据作为最终模式结果（集成） Stacked generalization Stacked generalization was introduced by Wolpert in a 1992 paper, 2 years before the seminal Breiman paper “Bagging Predictors“. Wolpert is famous for another very popular machine learning theorem: “There is no free lunch in search and optimization“. The basic idea behind stacked generalization is to use a pool of base classifiers, then using another classifier to combine their predictions, with the aim of reducing the generalization error. Let’s say you want to do 2-fold stacking: Split the train set in 2 parts: train_a and train_bFit a first-stage model on train_a and create predictions for train_bFit the same model on train_b and create predictions for train_aFinally fit the model on the entire train set and create predictions for the test set.Now train a second-stage stacker model on the probabilities from the first-stage model(s).A stacker model gets more information on the problem space by using the first-stage predictions as features, than if it was trained in isolation. It is usually desirable that the level 0 generalizers are of all “types”, and not just simple variations of one another (e.g., we want surface-fitters, Turing-machine builders, statistical extrapolators, etc., etc.). In this way all possible ways of examining the learning set and trying to extrapolate from it are being exploited. This is part of what is meant by saying that the level 0 generalizers should “span the space”. […] stacked generalization is a means of non-linearly combining generalizers to make a new generalizer, to try to optimally integrate what each of the original generalizers has to say about the learning set. The more each generalizer has to say (which isn’t duplicated in what the other generalizer’s have to say), the better the resultant stacked generalization. Wolpert (1992) Stacked Generalization Blending Blending is a word introduced by the Netflix winners. It is very close to stacked generalization, but a bit simpler and less risk of an information leak. Some researchers use “stacked ensembling” and “blending” interchangeably. With blending, instead of creating out-of-fold predictions for the train set, you create a small holdout set of say 10% of the train set. The stacker model then trains on this holdout set only. Blending has a few benefits: It is simpler than stacking.It wards against an information leak: The generalizers and stackers use different data.You do not need to share a seed for stratified folds with your teammates. Anyone can throw models in the ‘blender’ and the blender decides if it wants to keep that model or not.The cons are: You use less data overallThe final model may overfit to the holdout set.Your CV is more solid with stacking (calculated over more folds) than using a single small holdout set.As for performance, both techniques are able to give similar results, and it seems to be a matter of preference and skill which you prefer. I myself prefer stacking. If you can not choose, you can always do both. Create stacked ensembles with stacked generalization and out-of-fold predictions. Then use a holdout set to further combine these models at a third stage. Stacking with logistic regression Stacking with logistic regression is one of the more basic and traditional ways of stacking. A script I found by Emanuele Olivetti helped me understand this. When creating predictions for the test set, you can do that in one go, or take an average of the out-of-fold predictors. Though taking the average is the clean and more accurate way to do this, I still prefer to do it in one go as that slightly lowers both model and coding complexity. http://mlwave.com/kaggle-ensembling-guide/上文提到了自动选择多个模型并建立集成模型，并获得了良好的效果。缺限可能在于预测所花时间较长，模型较复杂。]]></content>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kaggle小试2017-03-06]]></title>
    <url>%2F2017%2F03%2F06%2F2017-03-06%2F</url>
    <content type="text"><![CDATA[kaggle注册了kaggle账户，用的是yahoo.com的邮箱，用QQ邮箱注册时收到的验证链接打开后显示不了验证码。看了TITANIC的示例数据及指导，下载安装WIN7 64位的 XGBOOST参考GITHUB上的安装指引。This page gives instructions on how to build and install the xgboost package from scratch on various systems. It consists of two steps: First build the shared library from the C++ codes (libxgboost.so for linux/osx and libxgboost.dll for windows).Exception: for R-package installation please directly refer to the R package section.Then install the language packages (e.g. Python Package).Important the newest version of xgboost uses submodule to maintain packages. So when you clone the repo, remember to use the recursive option as follows. 安装git for windows,打开git-bash cd c: git clone --recursive https://github.com/dmlc/xgboost For windows users who use github tools, you can open the git shell, and type the following command. cd xgboost git submodule init git submodule update Building on WindowsXGBoost support both build by MSVC or MinGW. Here is how you can build xgboost library using MinGW. After installing Git for Windows, you should have a shortcut Git Bash. All the following steps are in the Git Bash. In MinGW, make command comes with the name mingw32-make. You can add the following line into the .bashrc file. alias make=&apos;mingw32-make&apos; To build with MinGW cp make/mingw64.mk config.mk; make -j4 MinGW尝试，不成功。 To build with Visual Studio 2013 use cmake. Make sure you have a recent version of cmake added to your path and then from the xgboost directory: mkdir build cd build cmake .. -G&quot;Visual Studio 12 2013 Win64&quot; This specifies an out of source build using the MSVC 12 64 bit generator. Open the .sln file in the build directory and build with Visual Studio. To use the Python module , you can copy libxgboost.dll intoc:\xgboost\python-package\xgboost. python setup.py install 安装成功。ipython import xgboost ok.ipython notebook 默认目录是 我的文档 根据教程做出的数据上传到KAGGLE评分0.78，排名2000多，呵呵太过复杂的模型的泛化能力不一定就更好。 再看了一下BOSTON的数据及一个讨论话题，回归问题与目前工作更相关，还是挺有意思的。]]></content>
  </entry>
  <entry>
    <title><![CDATA[现在开始记录]]></title>
    <url>%2F2017%2F03%2F05%2F2017-03-05%2F</url>
    <content type="text"><![CDATA[Quick Start用微信连接ECT，比用APP连接更稳定。 进口产品论证流程： 下载表格，填写仪器性能需求。进行专家论证。结果返回后打印盖章。交财局审批，放在总务处靠门口的文件夹里。 文件上交时注意装订，或用信封装好。 扫描时可把多页文件一次双面扫描成PDF. 软件著作权申请 源代吗，技术说明，用户使用说明，单位资料 联系公司办理。根据要求准备材料。 发送材料，收到发票，办理转账，35天后收材料。 不同作者的书的水平还是有区别的。这几天看了一本翻译版的书籍，文中的术语有几个让人看着比较别扭，整个过程看得人比较难受。。看完一遍感觉多是在翻译软件的使用手册，不过也算是打基础吧。今天看另外一本书，作者是国人，所用术语相比上一本书要准确很多，知识介绍也比较有条理，看上去就感觉很好。今天学到许多有用的知识，《python机器学习及实践》 numpy 里面repeat与tile，在构建array时很有用。 sklearn模型数据预处理时可以先实例化对象（如ss=StandScaler()），对训练数据fit_transform后再对测试数据进行transform. ss=StandScaler() ss.fit_transform(data_train) ss.transform(data_test) 几个有用的术语：准确率：accuracy;精确率:precision；召回率:recall;f1得分：f1-score; 分类模型：线性分类，单一决策树，朴素贝叶斯，K邻近，随机森林（集合模型常用的基线系统），梯度上升，支持向量机 梯度法迭代渐近估计参数法，SGA用于目标最大化及SGD用于目标最小化，回归问题中是目标最小化。 回归模型：线性回归，支持向量机，K邻近回归， 用inverse_transform还原预测结果。 R－squared:衡量模型回归结果的波动可被真实值验证的百分比,Page 69 核函数配置，通过某种函数计算，将原有特征映射到更高维度的空间，从而尽可能达到新的高维度特征线性可分的程序。结合支持向量机的特点，这种高维度线性可分的数据特征恰好可以发挥其模型优势。 2017-02-22上午测试了把SCRAPY 安装到服务器上，运行不成功，报错type error float is not iterable.以为是AQI工程的问题，换了EXAMPLE的工程还是报同样的错误。可能是ANACONDA的版本问题。在WIN7 64位系统上运行是没问题的。发现ANACONDA3更新到PYTHON3.6版了。下载下来装到二楼看看行不行。MARKDOWN编辑的HAROOPAD在WIN系统上不太方便，下载了个MARKDOWNPAD看上去还行。二楼的电脑安装PY36的ANACONDA报错 DLL LOADED FAILED，下载安装了Win32OpenSSL-1_0_2k.exe后成功。在BAT文件前加上下面这几句就可以自动最小化窗口。 @echo off %1(start /min cmd.exe /c %0 :&amp;exit) echo 你的代码写在这下面,最小化运行至任务栏。 pause ANACONDA运行SCRAY失败报错，type error float object is not iterable.试一试PY27版本的也不行，新建一个项目直接运行也显示上述错误。there is no answer for the error online. 计算代码长度的工具cloc-1.64.exe]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2016%2F09%2F07%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
  <entry>
    <title><![CDATA[利用github and Hexo建立个人blog]]></title>
    <url>%2F2016%2F08%2F27%2Fmy-new-post%2F</url>
    <content type="text"><![CDATA[Hexo 是一个快速、简单且功能强大的 Node.js 博客框架，可以方便的生成静态网页托管在github和Heroku上。 GitHub Pages 可以被认为是用户编写的、托管在github上的静态网页。由于它的空间免费稳定， 可以用于介绍托管在github上的Project或者搭建网站。 gp 生成的网站的默认域名是 username.github.io 或者 username.github.io/project-name ，但gp是支持自定义域名的： Custom Domain Name 。购买域名之后，可以和默认的二级域名进行绑定 由于 Hexo 是基于 Node ，安装前要先安装 Node.js。上官网下载安装。 注册github，建立username.github.io的项目用于管理blog相关资料。 下载安装github for windows,用git shell安装Hexo 有些教程要先CD到根目录，这得看你的github for windows的安装位置，直接启动git shell安装就行了。进git shell测试连github 12ssh -T git@github.comnpm install hexo-cli -g cd到Hexo项目文件夹，在github文件夹下面。运行 1234hexo init Hexonpm installhexo generatehexo server 这时端口4000被打开了，我们能过浏览器打开地址， http://localhost:4000/ 或者 http://0.0.0.0:4000 。在Hexo的 _config.yml 中修改deploy 123456789101112131415161718192021222324# Deployment## Docs: http://hexo.io/docs/deployment.htmldeploy:type: gitrepo: https://github.com/dwqs/username.github.io.gitbranch: master然后还要安装 hexo-deployer-git ：npm install hexo-deployer-git -S最后利用hexo指令发布到github：hexo d//same ashexo deployHexo常用命令：使用git shell ，cd Hexo项目文件夹hexo new "article title"hexo s == hexo serverhexo n == hexo newhexo cleanhexo g == hexo generatehexo d == hexo deploy 使用hexo n “filename”建立新blog，用Haroopad编辑mark dwon模式的md文件，保存后刷新可看到最新的blog]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>